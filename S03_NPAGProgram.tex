%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%                                                                  Section NPAG
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{NPAG Implementation (NPAG - Algorithm 1)}
\label{NPAG}

NPAG is a Fortran program consisting of a number of subroutines as described below.
%
The main program performs the Adaptive Grid (AG) method (consisting of expansion and compression algorithms) and calls the Primal-Dual Interior-Point (PDIP) subprogram.
% .  The AG program calls the Primal-Dual Interior-Point (PDIP)  subprogram.
The PDIP algorithm solves the maximization problem  of Eq. (\ref{Eqn:Likelihood002}) for a fixed grid and is  described precisely in the Appendix. 

For the purpose of this discussion, we can think of PDIP as a function  $\bm{\hat{\lambda}}$ from $\Theta^m$ into the set $S^m$ = $\{\bm{\lambda}  \in \mathbb{R}_{+}^{m}:  \sum_{k=1}^m  \lambda_k = 1 \}$
%$(m-1)$-dimensional simplex $S^{m}$, 
defined as follows:
%
If  $\bm{\phi} = (\bm{\phi}_1, ... , \bm{\phi}_m)$ then $\bm{\hat{\lambda}}(\bm{\phi})= (\hat{\lambda}_1, ... ,\hat\lambda_m)$
maximizes Eq. (\ref{Eqn:Likelihood002})  relative to the fixed set of grid points $(\bm{\phi}_1, ... , \bm{\phi}_m)$. 
In this case we write  $G$ = $(\bm{\phi},\bm{\hat{\lambda}}(\bm{\phi}))$  and  $l(G)$ = $l(\bm{\phi}, \bm{\hat{\lambda}}(\bm{\phi}))$.

In NPAG there are two types of grids: expanded and condensed. The expanded grids are the initial grid and the grids after Grid Expansion (Algorithm 2).
The condensed grids are generated by Grid Condensation (Algorithm 3). % Except for the last cycle, the likelihood calculation is done on the expanded grids.
Each cycle of NPAG begins with an expanded grid. The likelihood calculation is done on the condensed grids.

%%%
Now for the Adaptive Grid method.
%
Assume that $ \Theta$  is  a bounded $Q$-dimensional hyper-rectangle.
%
Initially we let  $\bm{\phi}^{0}_{expanded} = (\bm{\phi}^{0}_1, ... , \bm{\phi}^{0}_M)$ be the set of $M$ Faure grid points in $\Theta$, see
\cite{FaureSequence, InitFaureSequence, DoFaureSequence}.
%
Alternatively, we could initially let $\bm{\phi}^0_{expanded}$ be generated by a uniform distribution on  $\Theta$ or by a prior run of the program.

Remark. The Faure grid points for a hyper-rectangle $\Theta$ are a low-discrepancy set which in some sense optimally and uniformly covers $\Theta$.  In our implementation of NPAG, the Faure point sets come in discrete sizes which nest with each other.  (Allowable number of  points equals 2129,  5003, 10007,  20011,  40009, 80021, and multiples of 80021.) This nesting property  is useful for checking the optimality of  $F^{ML}$, see Section 4. We have found that replacing the initial Faure set by a set generated by a uniform distribution on  $\Theta$
increases the time to convergence but results in the same optimal distribution. 

%In Figure XXX, a side-by-side examplesof a Faure set on 
%$[0,1] \times [0,1]$ and a set generated by a uniform distribution
%are displayed.

Now set $G^0_{expanded}$ = $(\bm{\phi}^0,\bm{\hat\lambda}(\bm{\phi}^0))$.
%
Our approach is to generate a sequence of solutions $G^n$ to Eq. (\ref{Eqn:Likelihood002}) of increasingly greater likelihood,
where unless otherwise specified, $ G^n $ refers to the condensed grid at the $n^{th}$ cycle of the algorithm.
%, and stopping when evaluation of $G^{n}$ is negligibly different than evaluation of $G^{n-1}$.
%
%At that point, $G^n$ is considered a {\em potential} optimal solution to Eq. (\ref{Eqn:Likelihood002}), and $l(G^n)$ is saved as $F_0$.
%
%$\bm{\phi}^n$ is then used as a seed to generate a new $\bm{\phi}^0$, which is used to begin a new sequence.
%
If  $ G^n $ has log likelihood negligibly different than  $ G^{n-1} $, then $ G^n$  is considered the optimal solution to Eq. (\ref{Eqn:Likelihood002}) and is relabeled $F^{ML}$.
%
If not, then the process continues using the $\bm{\phi}^n$ as the new seed.
%
This loop is repeated until $F^{ML}$ is found.


The stopping conditions for NPAG are defined precisely in Algorithm 1.
If the stopping conditions are not met prior to a set maximum number of iterations, the program will exit after writing the last calculated  $G^n$ into a file.
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Grid Expansion (EXPAND - Algorithm 2)}
\label{SS:GridExpansion} 

The crux of the Adaptive Grid method is how to go from $G^0$ to $G^1$ or more generally, from $G^n$ to $G^{n+1}$.
%%%
%
The details of doing this are now explained roughly below and precisely in Algorithm \ref{OuterLoopOfNPAG}.
%see also Algorithms \ref{GridExpansionAlg} and \ref{Algorithm:PDIP}.

Let $Q$ be the dimension of $\Theta$.  Suppose at stage $n$ we have a grid of high-probability support points 
$\bm{\phi}^n$. We then add $2Q$ daughter points for each support point   $\bm{\phi}_k \in \bm{\phi}^n$. 
%%
The daughter points are the vertices of a small hyper-rectangle centered at each $\bm{\phi}_k$ with size proportional to the original size of the hyper-rectangle defining $\Theta$. The size of this small hyper rectangle decreases as the accuracy of the estimates increases. (See Algorithm 2.) 

Let $\bm{\phi}^{n+1}_{expanded}$ = $\bm{\phi}^n \cup \mbox{Daughter-Points}$.
Then the PDIP subprogram is applied to $\bm{\phi}^{n+1}_{expanded}$ resulting in the
new solution set $G^{n+1}_{expanded}$ =  $(\bm{\phi}^{n+1}_{expanded},\bm{\hat\lambda} (\bm  {\phi}^{n+1}_{expanded}  )  )$; 
see Algorithm \ref{OuterLoopOfNPAG}. The solution set $G^{n+1}_{expanded}$ is now ready for grid condensation.

%%% %%%
\subsection{Grid Condensation (CONDENSE - Algorithm 3)}
The above solution set $G^{n+1}_{expanded}$   may have many support points with very low probability.
We remove all support points which have corresponding probability less than 
%( $\bm{\lambda} > (\max\bm{\lambda})\Delta_\lambda $ )}$
$(\max\bm{\lambda}) \Delta_{\bm{\lambda}}$, where $\bm{\lambda}$ is the vector of current probabilities and  the 
default for  $\Delta_{\bm{\lambda}}$  is $10^{-3}$.  (Note that at this point the remaining probabilities are not normalized.)
%
The probabilities of the remaining support points are normalized by a second call to the PDIP subprogram.
%
This second call to PDIP is very fast.
%
The likelihood associated with these remaining support points and normalized probabilities is then used to update the program control parameters and check for convergence (Algorithm \ref{OuterLoopOfNPAG} and Section \ref{SS:Convergence}).
If convergence is attained, then the output of this second call to PDIP provides the support points and probabilities of the final solution.
If convergence is not attained, then the remaining support points are sent to the Grid Expansion subprogram (Algorithm 2), initializing the next cycle.
%

At the end of the program, the output of this second call to PDIP provides the location and weights of the final solution.

\subsection{PDIP Subprogram - See Appendix A}
\label{SS:PDIP_subprogram}

%
The PDIP subprogram finds the optimal solution to Eq. \ref{Eqn:Likelihood002} with respect to 
$\bm{\lambda}$ for fixed $\bm{\phi}$. 
%$\bm{\hat{\lambda}}$ 
PDIP  employs a primal-dual interior-point method that uses a relaxed Newton method to solve the corresponding Karush-Kuhn-Tucker equations.
(See Eqs. 14 -17 of Appendix A.)


 For any $\bm{Y}$=($\bm{Y}_1,...,\bm{Y}_N$) and any $\bm{\phi}$=($\bm{\phi}_1,...,\bm{\phi}_K) \in
\Theta^K$, the input to the  PDIP  subprogram is the 
$N \times K$ matrix  $\left\{ p(\bm{Y}_i\vert \bm{\phi}_k)\right\}$. The output consists of the optimal weights $\bm{\hat{\lambda}}(\bm{\phi})$ and the corresponding log-likelihood  $\l(\bm{\hat{\lambda}}(\bm{\phi}),\bm{ \phi}) $.
An in-depth description of the PDIP algorithm and its implementation is presented in Appendix A.
See also \cite{Bell2012b:Online, LAPK-2014-01, Baek2006}.



%%% %%%
\subsection{NPAG Stopping Conditions}Algorithm 
\label{SS:Convergence}

%%%
As explained above, a {\em potential} solution to $F^{ML}$ is not accepted as a global optimum until successive sequences of $G^n$ produce final distributions evaluating to sufficiently close log likelihood.
The various upper and lower bounds $\Delta$ for NPAG control and stopping conditions are defined below and are used in Algorithms 1, 2,  and 3.
%

\begin{enumerate}

\item [$\Delta_L$] Primary upper bound on the allowable difference between two successive estimated Log-Likelihoods; the default initialization is $10^{-4}$.

\item [$\Delta_F$] Secondary upper bound on the allowable difference between two successive estimated Log-Likelihoods of {\em potential} $F^{ML}$; the default initialization is $10^{-2}$.

\item[$\Delta_e$] Sets an upper bound on the accuracy variable $eps$ of Algorithm 1.   The default initialization for $\Delta_e$ is $10^{-4}$.  The default initialization for $eps$ is $0.2$  and is stepped down until $eps \leq  \Delta_e$

$\Delta_F$ and $\Delta_e$ define the two  stopping conditions for Algorithm 1.

%%
\item [$\Delta_D$] Sets a lower bound on how close two support points can get; the default initialization is $10^{-4}$.

\item[$\Delta_\lambda$] Sets a lower bound factor on the probabilities of the weights $\lambda$; the default initialization is $10^{-3}$.
%

\end{enumerate}
%The program exits if $\Delta_G < 0.0001$ and $\vert\vert l(G^b) - l(G^{b-1} \vert\vert < \Delta_F$.
%See Algorithm \ref{OuterLoopOfNPAG}.


\subsection{Calculation of $p(\bm{Y}_i\vert \bm{\phi}_k)$}
%
Given observations $\bm{Y}_i$, $i =  1,...,N$ and grid points  $\bm{\phi}_k$, $k =  1,...,K$,  the PDIP subprogram only depends on the $N \times K$ matrix 
$\left\{ p(\bm{Y}_i\vert \bm{\phi}_k)\right\}$. NPAG can be used for any problem  once this matrix is defined.   However, the default setting of NPAG is for the problem of population pharmacokinetics. For   a good background of population pharmacokinetics see \citet{DandG1995,DandG2003}.
%

In population pharmacokinetics, generally  $\bm{Y}_i=( \bm{y}_{i,1}, ... , \bm{y}_{i,M})$ is a matrix of vector observations for the i-th subject. 
%
Since NPAG allows multiple outputs, each $\bm{y}_{i,m}$ is itself a $q$-dimensional vector
$\bm{y}_{i,m}$ =$(y_{i,m,1}, \cdots,y_{i,m,q} )$. % $\bm{y}_{i,m}$ =$(y_{i,m}^1, \cdots,y_{i,m}^q )$.
%
The observations $y_{i,m,j}$, % $y_{i,m}^j $,
are then typically given by a regression equation of the form:
\begin{align}
	%%% %Y_i  &=  (  y_{i,1}, ... , y_{i,M}  ) \nonumber \intertext{where,}
	% y_{i,m}^j &= f_{i,m}^j(\bm{\theta}_i) + \nu_{i,m}^j \label{Eqn:f},\; j=1,\cdots,q \\
	y_{i,m,j} &= f_{i,m,j}(\bm{\theta}_i) + \nu_{i,m,j} \label{Eqn:f},\; j=1,\cdots,q \\
	 % \nu_{i,m}^j &\sim  N(0,(\sigma_{i,m}^j(\bm{\theta}_i))^2) \nonumber \\
	 \nu_{i,m,j} &\sim  N(0,(\sigma_{i,m,j}(\bm{\theta}_i))^2) \nonumber \\
	 %\chi_i & \mbox{ are observed parameters or covariates specific for $Y_i$} \nonumber \\
	 \bm{\theta}_i & \mbox{ are unobserved parameters specific for $\bm{Y}_i$} \nonumber
\nonumber
\end{align}
In the above Eq. 5, $f_{i,m,j}$ % $f_{i,m}^j$
is a known nonlinear function depending on the model structure, the dosage regimen, the sampling schedule, all covariates and of course the subject-specific parameter vector $\bm{\theta}_i$. 
%%(This corrsponds to the case where there are $q$ outputs.) 
Except for simple models, $f_{i,m,j}$
requires the solution of (possibly nonlinear) ordinary differential equations.

%%For simplicity, assume 9/23/17
In the current implementation of NPAG, it is assumed that the $( \bm{y}_{i,1}, ... , \bm{y}_{i,M})$ are independent.   %$\Sigma_i = \mbox{diag}(\sigma_{i,1}^2, ... , \sigma_{i,M}^2)$ for simplicity,
Then
\begin{equation}
	% \Psi\left( i,k \right) &=
		 p( \bm{Y}_i \vert \bm{\phi}_k) % \nonumber \\
	% \Psi\left( i,k \right) & 
	= 
            \frac{ \exp\left(
	      {\displaystyle
	         - \frac{1}{2} \sum_{m = 1}^M	            
                         ( \bm{y}_{i,m}  -  \bm{f}_{i,m}( \bm{\phi}_k ) )  \bm\Sigma_{i,m}^{-1}( \bm{\phi}_k) 
( \bm{y}_{i,m}  -  \bm{f}_{i,m}( \bm{\phi}_k )) ^T
                  }
               \right)}
               {\prod_{m = 1}^M \sqrt { (2\pi)^q \det \bm{\Sigma}_{i,m}( \bm{\phi}_k)} } \label{eqnmetric}
\end{equation}

\noindent where $\bm{f}_{i,m}=(  f_{i,m,1}, ... ,f_{i,m,q})$ % $\bm{f}_{i,m}=(  f_{i,m}^1, ... ,f_{i,m}^q)$
and $\bm{\Sigma}_{i,m}$ = $diag(\sigma_{i,m,1}^2, ..., \sigma_{i,m,q}^2)$. % $\bm{\Sigma}_{i,M}$ = $diag((\sigma_{i,m}^1)^2, ...,(\sigma_{i,m}^q)^2)$.
%
For the purposes of matrix multiplication in Eq. 6 ,we think of $\bm{y}_{i,m}$ and $\bm{f}_{i,m}$ as $q$-dimensional row vectors.

To complete the description of Eq. 6 we need to model the standard deviation terms $\sigma_{i,m,j}$ of the assay noise. % $\sigma_{i,M}^j$ of the assay noise. 
%Consider equation \ref{eqnmetric} and again assume that
%%$\Sigma_i = \mbox{diag}(\sigma_{i,1}^2, \sigma_{i,2}^2, ... , \sigma_{i,M}^2$. 
In our implementation of NPAG, four different models are allowed. Let
\begin{equation}
\alpha_{i,m,j}( \bm{\phi}_k ) = c_0 + c_1 f_{i,m,j}( \bm{\phi}_k )  + c_2 f_{i,m,j}^2( \bm{\phi}_k) + c_3 f_{i,m,j}^3(\bm{\phi}_k)
\end{equation}
and set
\begin{align}
% \noalign{\noindent Each measurement is then assumed to be $\sim \mathcal{N}(y_{im}, \sigma)$, where } \nonumber \\
\sigma_{i,m,j} & =
\begin{cases}
\alpha_{i,m,j}& \mbox{assay error polynomial only} \\
\gamma \alpha_{i,m,j}& \mbox{multiplicative error} \\
\sqrt{ \alpha_{i,m,j}^2 + \gamma^2} & \mbox{additive error} \\ % \footnote{Additive noise is usually notated by $\lambda$.}} \\
\gamma & \mbox{constant level of error}
\end{cases} \label{NoiseModel}
\end{align}






%

The parameter $\gamma$ in Eq. 8 is a variance factor.  Artificially increasing the variance during the first several cycles of NPAG increases the likelihood for each $\bm{\phi}$, allowing the algorithm to use these cycles to find a better initial state from which to begin optimization.
NPAG also has an option to ``optimize" $\gamma$. This changes NPAG from a nonparametric method to  a ``semiparametric" method and will not be discussed here. The interested reader can consult \cite{LAPK-2014-01}.
%
%

Next if $c_0=0$ in Eq. 7,  then $\alpha_{i,m,j}$ can become very small for certain values of $\bm{\phi}$ that in early iterations can be far from optimal. This in turn causes numerical problems as the likelihood is infinite if $\sigma_{i,m,j}=0$.
One way to avoid this problem is to
take $\sigma_{i,m,j} = constant $. Another way
would be to  assume that $\alpha_{i,m,j}$ is {\em known} and is given by
\begin{equation}
\alpha_{i,m,j} = c_0 + c_1y_{i,m,j} + c_2 y_{i,m,j}^2 + c_3 y_{i,m,j}^3 \\
\end{equation}
That is, to approximate $\sigma$ by using a polynomial of the observed values rather than model predicted values.
In our experience with  NPAG, 
the approximation of Eq. 9 is useful for ensuring computational stability (especially during the early cycles of the algorithm). However, from a theoretical perspective, this change violates the conditions of maximum likelihood and will not be discussed here. Again the interested reader can consult \cite{LAPK-2014-01}.
%%see XXX Paper by Beal XXX.
%%and is discussed in section \ref{Section:Conclusion}. 
%For all the above noise models, the assay polynomial coefficients must be put into the algorithm by the user. A program for estimating the assay error polynomial from patient observations or from assay validation data supplied by the analytic laboratory is available at {\tt www.lapk.org}.
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%
%%%In Algorithm  \ref{OuterLoopOfNPAG}, a call is made to the subprogram AdjustGamma.
%%
%%$\gamma^2$ is a multiplicative factor on the regression model error variance (Eq. \ref{Eqn:f}).
%%%
%%%Artificially increasing the variance during the first several cycles increases the likelihood of each $\varphi \in G'$, allowing the algorithm to use these cycles to find a better initial state from which to begin optimization.
%%%%
%%%In practice, $\gamma$ is initialized to a value of 10.
%%%%
%%%Typically, $\gamma$ relaxes quickly toward 1.
%%%
%%However, poorly sampled data (e.g. in the presence of large process noise) typically result in $\gamma$ relaxing to a value much higher than 1.
%%XXX Need few remarks on how gamma is adjusted XXX
%%Optimization of $\gamma$ is explained in algorithm \ref{GammaOpt}.
%%%% END FILE
